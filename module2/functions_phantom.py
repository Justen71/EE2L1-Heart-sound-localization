# -*- coding: utf-8 -*-
"""MusicAlgorithm_Explanation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NaSKNGwfjRFAj9qC3vCc-gVm1EBoR4vC

# Localization of Heart Sound

To start, some libraries have to be imported
"""

from scipy import signal
import numpy as np
import matplotlib.pyplot as plt
import math
import scipy.linalg as LA
from numpy import genfromtxt

"""
Now we will create a function called the steering vector. The steering vector function creates the weight vector for a certain location. It takes as input the location and as output a vector with the weights. First it calculates the distance between the source and every microphone and based on that the phase shift and amplitude is calculated.
"""

def steering_vector(x, y, z, freq, v_s, positions):
    """
    Create the steering matrix, this is, the expected phase offsets of the microphones, for a given source location.

    Parameters:
    - x, y, z (float): Source location coordinates
    - freq (float): Frequency of the signal
    - v_s (float): Speed of sound
    - positions (array): Positions of microphones

    Returns:
    - rk (array): Steering matrix
    """
    source = [x, y, z]   # Source location of the signal in euclidean space
    dist = []
    for i in range(len(positions[0])):
        dist.append(math.dist([positions[0][i], positions[1][i], positions[2][i]], source))  # Distance between point and microphone
    rk = np.array(dist)*2*np.pi/(v_s/freq)  # Phase (transformation of microphone distance to phase delay)
    rk = np.exp(np.multiply(rk, -1j))       # Setting the phase shift in the exponent
    rk = rk/dist                            # Magnitude scaled to the expected magnitude
    rk = rk/sum(abs(rk))                    # Normalized weight vector (this step is needed to avoid the output of locations far away to give a large value)
    return rk

"""<font color='red'> **Additions needed** </font>
Some optimization can be done: is the for loop necessary, can we make it more clear to students?

Comments are very clear: almost all of the code can be left out to let the students figure out themselves using these comments.

## Fourier Transform

**Narrowband Assumption**
<br>
In the example that was given on the steering vector, there was a signal of only 1 frequency, namely 100Hz, which means it is narrowband: the frequencies in the signal lie in a small frequency band. But if the signal is wideband and has varying frequencies each has a different phase shift. It is then not possible to create a weight vector that works for all frequencies.

Luckily, we have learned how to extract different frequencies in a signal using the fourier transform. If we use the short time fourier transform (STFT) we can seperate the different frequencies and create a steering vector for every frequency. The STFT is used instead of DFT because the data can be non-stationary, which means the signal changes in time. The window size (n) defines the amount of samples per window. This value also influences the size of the frequency bins. For example taking n = 100 for f_sample = 1 Hz will calculate frequency contents in multiples of 1/100 Hz: [0.01Hz, 0.02Hz ...].

To satisfy the narrowband assumption the bandwidth in radians multiplied with the delay should be much smaller than $2\pi$. So $B << \frac{2\pi}{\tau}$. The bandwidth is equal to f_sample/n, because the frequency contents are calculated in multiples of f_sample/n.

<font color='red'> **Additions needed** </font>
Perhaps some images of the narrowband assumption can be useful, but maybe this is too much extra.

If the STFT is treated in the earlier labs for this project (I think Labday 2 of Telecom A) we can leave that as is. If not some more help for this could be useful.

### Code

Now we will create the bins function that calculates the STFT for every microphone. It is important to define the mode as complex, so the output will contain the phase information of the frequency bins. For every bin the covariance matrix will be calculated, the use of the covariance matrix will be further explained in the next section.
"""

def bins(data, fsample, nperseg, n_freq):
    """
    Calculate the covariance matrices for the frequency bins.

    Parameters:
    - data (array): Input signal data
    - fsample (int): Sampling frequency
    - nperseg (int): Length of each segment for STFT
    - n_freq (int): Number of frequencies to select

    Returns:
    - f_sel (array): Selected frequency bins
    - R_hat (array): Covariance matrices per frequency bin
    """
    bin_m = []
    NrChn = len(data)

    # Find the frequency spectrum per microphone
    for i in range(NrChn):
        f, t, Sxx = signal.spectrogram(data[i], fsample, mode='complex', nperseg=nperseg) # Spectrogram matrix (fft) (Window of 128, with 50% overlap, Hann window), which is a complex nxm matrix where n is the frequency bin and m the time bin
        bin_m.append(Sxx)         # List of the stft for every microphone

    # Calculate the covariance matrix for every frequency bin
    R_hat = []
    f_sel = f[0:n_freq]           # Selection of frequencies with a high signal power
    for j in range(len(f_sel)):
        X = []
        for i in range(NrChn):
            X.append(bin_m[i][j]) # List with the stft of every microphone for a specific frequency bin
        R_hat.append(np.cov(X))   # Covariance matrix for a specific frequency bin, which indicates the variance between the microphones
    R_hat = np.array(R_hat)
    return f_sel, R_hat

"""<font color='red'> **Additions needed** </font>
- If we give a more clear instruction of what this block is supposed to do I think we can leave a large part of it to the students. Specifically: the students can probably figure out how to find the values of the n_freq number of frequencies in the spectrogram and how to

## Covariance Matrix

For every frequency bin we will calculate the covariance matrix. In the frequency domain a delay becomes a phase shift. The covariance between microphones shows the phase and amplitude differences between the micophones. If **x** is zero mean the covariance is equal to the correlation, which can be calculated with the following formula where **x** is a column vector:

cov(**x**) = $\pmb{x}$ * $\pmb{x}^{H}$

for a vector x with 2 microphones the matrix would look like this:
$\begin{pmatrix}
x_1x_1^* & x_1x_2^* \\
x_2x_1^* & x_2x_2^*
\end{pmatrix}
$

We are only interested in the first column. This column already gives all the amplitudes and phases of the microphones. For example, if the first column contains the following elements:
$[0.6\exp{0.2}, 0.2\exp{-0.3}]^T$ the phase difference is 0.5 and the amplitude of $m_2$ is 1/3 of $m_1$. Now take the inner product of the conjugate of the steering vector with this vector. The result should be the highest for the steering vector in the source location. Because the steering vector should have the same phase differences.

<font color='red'> **Additions needed** </font>
Considering that this is the first time that students see the covariance matrix I think it is important to maybe be a bit more clear with the introduction: perhaps link it to covariance in general from Probability and Statistics so people can get more intuition from it.

# Localization Algorithms

## MVDR

First we will implement the MVDR algorithm. The formula to calculate the power in a certain location is equal to:  

$P_{MVDR} = \frac{1}{e^HR^{-1}e}$ [\[1\]](https://pysdr.org/content/doa.html)

Where e is the steering vector and $R^{-1}$ is the covariance matrix. Because the frequency of the signal is wideband the raw autocorrelation vector can not be used. Instead The autocorrelation on the frequency bins of the signal is used. Because this makes the covariance matrix complex the absolute value of the estimated MVDR power is used:

$P_{MVDR} = \frac{1}{|e^HR^{-1}e|}$

<font color='red'> **Additions needed** </font>
- I really recommend to either give some more insight into the functionality of the algorithm or to derive it yourself.
"""

def mvdr(Cov, freq, v_s, positions, area, resolution):
    """
    Perform the MVDR algorithm for source localization.

    Parameters:
    - Cov (array): Covariance matrix
    - freq (float): Frequency of the signal
    - v_s (float): Speed of sound
    - positions (array): Positions of microphones
    - area (array): limits of the area to be analyzed
    - resolution (array): step size for steering vector

    Returns:
    - xsearch, ysearch, zsearch (arrays): Search coordinates
    - Z (array): MVDR spectrum
    """
    Cov_inv = np.linalg.inv(Cov) # Inverse of the covariance matrix

    xsearch = np.arange(-area[0], area[0], resolution[0])     # x values to search
    ysearch = np.arange(-area[1], area[1], resolution[1])     # y values to search
    # zsearch = np.arange(0.03, area[2], resolution[2])  # z values to search
    # zsearch = np.arange(0.03, area[2], resolution[2])  # z values to search
    zsearch = [0.075]  # z values to search
    x_size = len(xsearch)
    y_size = len(ysearch)
    z_size = len(zsearch)

    # Initialize variables
    Z = np.empty([x_size, y_size, z_size])

    # Calculate MVDR for every location in the grid
    for i_n, i in enumerate(xsearch):
        Z_j = np.empty(y_size)
        for j_n, j in enumerate(ysearch):
            Z_k = np.empty(z_size)
            for k_n, k in enumerate(zsearch):
                e = np.reshape(steering_vector(i, j, k, freq, v_s, positions), (-1, 1)) # Steering vector
                Z[i_n, j_n, k_n] = abs(1/(e.conj().T @ Cov_inv @ e).squeeze())          # Power of MVDR
    return xsearch, ysearch, zsearch, Z

"""<font color='red'> **Additions needed** </font>

"""

def mvdr_extended(f_sel, R_hat, v_s, M_positions, area, resolution):
    """
    Extended MVDR algorithm summing results across frequencies.
    """
    Z = 0
    for i in range(len(R_hat)):
        xsearch, ysearch, zsearch, Z_int = mvdr(R_hat[i], f_sel[i], v_s, M_positions, area, resolution)
        Z = Z + Z_int
    Z = np.array(Z)
    return xsearch, ysearch, zsearch, Z

"""## MUSIC

Now we will implement the multiple signal classification (MUSIC) algorithm for localization. In short, the MUSIC algorithm works in the following way:

**Eigenvector Decomposition**
Because the covariance matrix is symmetric the eigenvectors are orthogonal. Every column of the covariance matrix is equal to the same phase shift vector, but scaled by different values because for every column, a different microphone is taken as reference microphone.

The eigenvector of the covariance matrix is a vector containing the phase shift for every microphone. There are 2 eigenvectors for one signal, $e^{{-2\pi*t*f}}$ and $e^{{2\pi*t*f}}$. The following matrix shows the sorted eigenvalue decomposition of the covariance matrix for a signal with two sources, where $σ_w^2$ is the variance of the noise:

![](https://drive.google.com/uc?export=view&id=1hXkRDIhFOmoVMP6S9CLxcvqL3OVhiUxs)

The eigenvectors that do not correspond to a signal span the noise subspace. In MUSIC the sum of the dot product of every noise eigenvector with the steering matrix is calculated. Which can be seen in the following formula, where e is the steering vector. This formula has a peak at the correct steering. This is because the noise subspace should be orthogonal to the signal eigenvectors, and thus to the steering vector. So the inner product between the two should be 0.

![](https://drive.google.com/uc?export=view&id=1el6hIaDQ_85RhmmhT7g88hros8QOtH-I)

**Multiple Frequencies**
Because our signal consists of multiple frequencies we first create a spectrogram for each microphone and take the covariance matrix for the microphones at specific frequencies. Take for example the following spectrogram. In the picture the frequency bin of 1030, where the time bins are outlined with black borders, will be selected and that row in the spectrogram will be taken. If we do this for every microphone and create the covariance matrix between microphones for the frequency bin of 1030, the result will again be a covariance matrix with eigenvectors corresponding to the array response. But now there is only one eigenvector per signal because the values of the covariance matrix are complex.

![](https://drive.google.com/uc?export=view&id=1ZjWlhu4tFGHlcxKNguJoB-wCBQ-e_L0Y)


This is done for every frequency bin and the MUSIC result of every frequency bin is summed. The result is a MUSIC algorithm that can handle wide band signals!

### Code
Now we will write the code of the music algorithm for a single frequency
"""
def print_eigenvalues(Cov):
    # Take the eigenvalue decomposition and sort the eigenvectors based on eigenvalue size
    D, E =  LA.eig(Cov)     # Eigenvalue decomposition
    D = np.real(D)          # Real part of the eigenvalues (because the covariance matrix is hermitian symmetric the eigenvalues should be real, but due to numerical errors it isn't)
    idx = D.argsort()[::-1]
    lmbd = D[idx]           # Vector of sorted eigenvalues
    return lmbd

def music(Cov, freq, v_s, positions, S_num, area, resolution):
    """
    Perform the MUSIC algorithm for source localization.

    Parameters:
    - Cov (array): Covariance matrix
    - freq (float): Frequency of the signal
    - v_s (float): Speed of sound
    - positions (array): Positions of microphones
    - S_num (float). number of sources to be localized
    - area (array): limits of the area to be analyzed
    - resolution (array): step size for steering vector

    Returns:
    - xsearch, ysearch, zsearch (arrays): Search coordinates
    - Z (array): MUSIC spectrum
    """

    # Take the eigenvalue decomposition and sort the eigenvectors based on eigenvalue size
    D, E =  LA.eig(Cov)     # Eigenvalue decomposition
    D = np.real(D)          # Real part of the eigenvalues (because the covariance matrix is hermitian symmetric the eigenvalues should be real, but due to numerical errors it isn't)
    idx = D.argsort()[::-1]
    lmbd = D[idx]           # Vector of sorted eigenvalues
    E = E[:, idx]
    En = E[:, S_num:len(E)] # Noise eigenvectors


    # Define the grid in which we want to search for the signal
    xsearch = np.arange(-area[0], area[0], resolution[0])     # x values to search
    ysearch = np.arange(-area[1], area[1], resolution[1])     # y values to search
    # zsearch = np.arange(0.03, area[2], resolution[2])  # z values to search
    zsearch = [0.075]  # z values to search

    x_size, y_size, z_size = len(xsearch), len(ysearch), len(zsearch)

    # Initialize ariables
    Z = np.empty([x_size, y_size, z_size])  # Matrix of coordinates searched

    # Calculate MUSIC for every location in the grid
    for i_n, i in enumerate(xsearch):
        Z_j = np.empty(y_size)
        for j_n, j in enumerate(ysearch):
            Z_k = np.empty(z_size)
            for k_n, k in enumerate(zsearch):
                p = np.reshape(steering_vector(i, j, k, freq, v_s, positions), (-1, 1))  # Steering vector
                p = np.conjugate(p)                                           # Conjugate of the steering vector
                chemodan = np.dot(np.transpose(p), En)                        # Dot product of the steering vector with every noise vector
                aac = np.absolute(chemodan)
                aad = np.square(aac)
                aae = 1/np.sum(aad, 1)                                        # MUSIC power for the corresponding (x, y, z) location
                Z_int = aae
                Z[i_n, j_n, k_n] = Z_int[0]

    Z = np.array(Z)
    Z = np.squeeze(Z)
    return xsearch, ysearch, zsearch, Z

"""The code for MUSIC so far calculates the result for only one frequency bin. Now we iterate over every frequency bin and sum the result of every bin."""

def music_extended(f_sel, R_hat, v_s, M_positions, S_num, area, resolution):
  """
  Extended MUSIC algorithm summing results across frequencies.
  """
  Z = 0
  for i in range(len(R_hat)):
    # Calculate music for a specific frequency bin
    xsearch, ysearch, zsearch, Z_int = music(R_hat[i], f_sel[i], v_s, M_positions, S_num, area, resolution)

    # Sum the power to the other conferences
    Z = Z + Z_int

  return xsearch, ysearch, zsearch, Z

"""## GD-MUSIC

An alternative implementation of the MUSIC algorithm is the GD-MUSIC, which performs better when close and coherent sources are present. Although various approaches have been reported, the most simple approach is implemented which modifies the algoirthm by multiplying the MUSIC spectra with the gradient of the MUSIC spectrum. The MUSIC spectra are just the result of the MUSIC algorithm without GD.

$\mathbf{P}_{GDM} = \nabla{\mathbf{P}_M} . \mathbf{P}_M$
"""

def gd_music(Cov, freq, v_s, positions, S_num, area, resolution):
    """
    Perform the MUSIC algorithm for source localization.

    Parameters:
    - Cov (array): Covariance matrix
    - freq (float): Frequency of the signal
    - v_s (float): Speed of sound
    - positions (array): Positions of microphones
    - S_num (float). number of sources to be localized
    - area (array): limits of the area to be analyzed
    - resolution (array): step size for steering vector

    Returns:
    - xsearch, ysearch, zsearch (arrays): Search coordinates
    - Z (array): MUSIC spectrum
    """

    # Take the eigenvalue decomposition and sort the eigenvectors based on eigenvalue size
    D, E =  LA.eig(Cov)     # Eigenvalue decomposition
    D = np.real(D)          # Real part of the eigenvalues. Because the covariance matrix is hermitian symmetric the eigenvalues should be real. But due to numerical errors it isn't.
    idx = D.argsort()[::-1]
    lmbd = D[idx]           # Vector of sorted eigenvalues
    E = E[:, idx]
    En = E[:, S_num:len(E)] # Noise eigenvectors


    # Define the grid in which we want to search for the signal
    xsearch = np.arange(0, area[0], resolution[0])     # x values to search
    ysearch = np.arange(0, area[1], resolution[1])     # y values to search
    zsearch = np.arange(0.03, area[2], resolution[2])  # z values to search
    x_size = len(xsearch)
    y_size = len(ysearch)
    z_size = len(zsearch)

    # Initialize variables
    Z = np.empty([x_size, y_size, z_size])
    Z_fin = np.empty([x_size, y_size, z_size])

    # Calculate MUSIC for every location in the grid
    for i_n, i in enumerate(xsearch):
        Z_j = np.empty(y_size)
        for j_n, j in enumerate(ysearch):
            Z_k = np.empty(z_size)
            for k_n, k in enumerate(zsearch):
                p = np.reshape(steering_vector(i, j, k, freq, v_s), (-1, 1))  # Steering vector
                p = np.conjugate(p)                                           # Conjugate of the steering vector
                chemodan = np.dot(np.transpose(p), En)                        # Dot product of the steering vector with every noise vector
                aac = np.absolute(chemodan)
                aad = np.square(aac)
                aae = 1/np.sum(aad, 1)                                        # MUSIC power for the corresponding (x, y, z) location
                Z_int = aae
                Z[i_n, j_n, k_n] = Z_int[0]

    Z = np.array(Z)
    Z = np.squeeze(Z)

    # Compute gradient of the MUSIC Power spectrum
    grad = np.gradient(Z, resolution[0], resolution[1], resolution[2])
    grad_mod = np.sqrt(np.square(grad[0])+np.square(grad[1])+np.square(grad[2]))
    Z_fin = grad_mod*Z

    return xsearch, ysearch, zsearch, Z, Z_fin

def gd_music_extended(f_sel, R_hat, v_s, M_positions, S_num, area, resolution):
  """
  Extended GD-MUSIC algorithm summing results across frequencies.
  """
  Z = 0
  Z_fin = 0
  for i in range(len(R_hat)):
    # Calculate music for a specific frequency bin
    xsearch, ysearch, zsearch, Z_int, Z_int_fin = gd_music(R_hat[i], f_sel[i], v_s, M_positions, S_num, area, resolution)

    # Sum the power to the other conferences
    Z = Z + Z_int
    Z_fin = Z_fin + Z_int_fint
  return xsearch, ysearch, zsearch, Z, Z_fin

"""# Plotting

Make a plot of the estimated location where every z coordinate gets its own 2D plot of x and y.
"""

def plot_layers(Z, xsearch, ysearch, zsearch, M_positions, sources):
  """
  Plot the MUSIC/MVDR spectrum for every layer (z value) and the microphone location
  """
  # Remove the dimensions with size 1
  K = np.squeeze(Z)
  Maximum = K.max()
  for i in range(len(K[0, 0])):
    # Printing the value of z (layer)
    print('z =', np.round(zsearch[i], decimals=3))

    # Plotting the MUSIC spectra
    plt.imshow(K[:, :, i], cmap = 'jet', vmin = 0, vmax = K.max(), interpolation='nearest', extent=(min(ysearch), max(ysearch), min(xsearch), max(xsearch)), origin = 'lower')
    plt.colorbar()

    # Plotting the positions of the microphones as red dots
    plt.scatter(M_positions[1], M_positions[0], c='r', s=40)
    plt.scatter(sources[0, 0:2], sources[1, 0:2], c='g', s=40)
    plt.legend(['Microphones', 'Sources'])
    plt.title('Source Location')
    plt.xlabel('Location y [meters]')
    plt.ylabel('Location x [meters]')
    plt.show()